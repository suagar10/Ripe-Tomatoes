{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Bidirectional, SpatialDropout1D, MaxPool1D, GlobalMaxPooling1D, Embedding, Input, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "admittedly , with a title like the mangler , you're not expecting the second coming of the godfather . or even halloween , for that matter . nevertheless , regardless of how silly the name sounds and how unpromising the cast looks , it's difficult to be prepared for something this atrocious . i won't claim this is the worst movie i've ever seen--there are plenty of worthy contenders for that honor--but it's certainly among a select group , right alongside such notables as split second and dr . giggles . i haven't spent much time in laundry factories , but i'm sure there aren't many of them with the near-gothic appearance of the blue ribbon in rivers valley , maine . people are being gobbled up by \" the mangler \" , the gargantuan machine that presses and folds sheets . he might well have been talking about the movie itself . anyway , it's not long before we learn that the mangler isn't the only machine to run amok . apparently , a refrigerator has developed some homicidal tendencies . imagine , if you will , the sight of two burly men wrestling with a refrigerator before one starts beating it up . actually , the overall plot isn't quite as lucid as the above description might suggest , but i've tried to condense things a little , leaving out , for example , all references to the \" missing finger club \" and the belladonna antacids that everyone seems to be taking . logic obviously isn't of concern to the mangler's creators . this film is a hodgepodge of the incoherent , the idiotic , and the simply awful . there's not one element of the production worthy of even faint praise . the acting is uniformly terrible . the direction is shoddy . the gore level is very high--the mangler doesn't leave much to the imagination , such as what happens when a person gets forced through a quarter-inch opening . why are good horror films so rare ? take one look at the lack of effort put into the mangler , and the answer will be obvious . it's a lot easier to come up with lifeless , hackneyed drivel like this than to expend any real effort . unfortunately , it's movies like this that give the genre a bad name . in this case , a very bad name . ultimately , sitting through the mangler is about as appealing as getting squeezed through the machine . \n0\n"
    }
   ],
   "source": [
    "reviews = []\n",
    "ratings = []\n",
    "\n",
    "dataset_dir = 'cornell_nlp_dataset/scaledata'\n",
    "folders = os.listdir(dataset_dir)\n",
    "for folder in folders:\n",
    "    fname = os.path.join(dataset_dir, str(folder)+'/'+'subj.'+str(folder))\n",
    "    f = open(fname)\n",
    "    data = f.read()\n",
    "    lines = data.split('\\n')\n",
    "    reviews.extend(lines)\n",
    "    reviews = reviews[:-1]\n",
    "    f.close()\n",
    "    fname2 = os.path.join(dataset_dir, str(folder)+'/'+'label.4class.'+str(folder))\n",
    "    f2 = open(fname2)\n",
    "    data2 = f2.read()\n",
    "    rating = data2.split('\\n')\n",
    "    ratings.extend(rating)\n",
    "    ratings = ratings[:-1]\n",
    "    ratings = [int(rat) for rat in ratings]\n",
    "    f2.close()\n",
    "print(reviews[1])\n",
    "print(ratings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No. of reviews in this dataset: 5006\n"
    }
   ],
   "source": [
    "print('No. of reviews in this dataset: %d' %len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No. of unique tokens: 46259\nShape of dataset:  (5006, 400)\nShape of labels:  (5006,)\n"
    }
   ],
   "source": [
    "maxlen = 400\n",
    "training_samples = 4006\n",
    "val_samples = 1000\n",
    "test_samples = 1000\n",
    "max_words = 15000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print('No. of unique tokens: %d' %(len(word_index)))\n",
    "\n",
    "dataset = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(ratings)\n",
    "\n",
    "print('Shape of dataset: ', dataset.shape)\n",
    "print('Shape of labels: ', labels.shape)\n",
    "\n",
    "indices = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "dataset = dataset[indices]\n",
    "labels = labels[indices]\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "x_train = dataset[:training_samples]\n",
    "y_train = labels[:training_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/suyash/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /home/suyash/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /home/suyash/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /home/suyash/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /home/suyash/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 400, 50)           750000    \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 400, 50)           0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, 394, 64)           22464     \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 78, 64)            0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 64)                24832     \n_________________________________________________________________\ndense (Dense)                (None, 4)                 260       \n=================================================================\nTotal params: 797,556\nTrainable params: 797,556\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Conv1D(64, 7, activation='relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 3004 samples, validate on 1002 samples\nWARNING:tensorflow:From /home/suyash/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nEpoch 1/40\n3004/3004 [==============================] - 9s 3ms/sample - loss: 1.3034 - acc: 0.3785 - val_loss: 1.2911 - val_acc: 0.3942\nEpoch 2/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 1.2720 - acc: 0.3991 - val_loss: 1.2852 - val_acc: 0.3942\nEpoch 3/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 1.2178 - acc: 0.4121 - val_loss: 1.1848 - val_acc: 0.4571\nEpoch 4/40\n3004/3004 [==============================] - 7s 2ms/sample - loss: 1.0253 - acc: 0.5113 - val_loss: 1.0690 - val_acc: 0.4870\nEpoch 5/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.8273 - acc: 0.5839 - val_loss: 1.0244 - val_acc: 0.5050\nEpoch 6/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.6775 - acc: 0.6771 - val_loss: 1.1338 - val_acc: 0.5070\nEpoch 7/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.5462 - acc: 0.7453 - val_loss: 1.2458 - val_acc: 0.5240\nEpoch 8/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.4415 - acc: 0.8389 - val_loss: 1.2376 - val_acc: 0.5619\nEpoch 9/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.3299 - acc: 0.8945 - val_loss: 1.3397 - val_acc: 0.5549\nEpoch 10/40\n3004/3004 [==============================] - 7s 2ms/sample - loss: 0.2369 - acc: 0.9311 - val_loss: 1.3861 - val_acc: 0.5489\nEpoch 11/40\n3004/3004 [==============================] - 7s 2ms/sample - loss: 0.1651 - acc: 0.9567 - val_loss: 1.5256 - val_acc: 0.5489\nEpoch 12/40\n3004/3004 [==============================] - 7s 2ms/sample - loss: 0.1177 - acc: 0.9667 - val_loss: 1.5206 - val_acc: 0.5549\nEpoch 13/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.1009 - acc: 0.9730 - val_loss: 1.7462 - val_acc: 0.5439\nEpoch 14/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.0887 - acc: 0.9730 - val_loss: 1.8066 - val_acc: 0.5309\nEpoch 15/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.0865 - acc: 0.9757 - val_loss: 1.7441 - val_acc: 0.5279\nEpoch 16/40\n3004/3004 [==============================] - 7s 2ms/sample - loss: 0.0627 - acc: 0.9850 - val_loss: 1.9026 - val_acc: 0.5439\nEpoch 17/40\n3004/3004 [==============================] - 7s 2ms/sample - loss: 0.0479 - acc: 0.9907 - val_loss: 1.8757 - val_acc: 0.5579\nEpoch 18/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.0398 - acc: 0.9903 - val_loss: 2.0351 - val_acc: 0.5329\nEpoch 19/40\n3004/3004 [==============================] - 6s 2ms/sample - loss: 0.0415 - acc: 0.9903 - val_loss: 2.0260 - val_acc: 0.5469\n"
    }
   ],
   "source": [
    "callbacks_list = [EarlyStopping(monitor='acc', patience=2), ModelCheckpoint(monitor='val_loss', filepath='trained_model.h5', save_best_only=True)]\n",
    "history = model.fit(x_train, y_train, epochs=40, batch_size=64, validation_split=0.25, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}