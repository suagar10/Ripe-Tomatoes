{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Bidirectional, SpatialDropout1D, MaxPool1D, GlobalMaxPooling1D, Embedding, Input, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "admittedly , with a title like the mangler , you're not expecting the second coming of the godfather . or even halloween , for that matter . nevertheless , regardless of how silly the name sounds and how unpromising the cast looks , it's difficult to be prepared for something this atrocious . i won't claim this is the worst movie i've ever seen--there are plenty of worthy contenders for that honor--but it's certainly among a select group , right alongside such notables as split second and dr . giggles . i haven't spent much time in laundry factories , but i'm sure there aren't many of them with the near-gothic appearance of the blue ribbon in rivers valley , maine . people are being gobbled up by \" the mangler \" , the gargantuan machine that presses and folds sheets . he might well have been talking about the movie itself . anyway , it's not long before we learn that the mangler isn't the only machine to run amok . apparently , a refrigerator has developed some homicidal tendencies . imagine , if you will , the sight of two burly men wrestling with a refrigerator before one starts beating it up . actually , the overall plot isn't quite as lucid as the above description might suggest , but i've tried to condense things a little , leaving out , for example , all references to the \" missing finger club \" and the belladonna antacids that everyone seems to be taking . logic obviously isn't of concern to the mangler's creators . this film is a hodgepodge of the incoherent , the idiotic , and the simply awful . there's not one element of the production worthy of even faint praise . the acting is uniformly terrible . the direction is shoddy . the gore level is very high--the mangler doesn't leave much to the imagination , such as what happens when a person gets forced through a quarter-inch opening . why are good horror films so rare ? take one look at the lack of effort put into the mangler , and the answer will be obvious . it's a lot easier to come up with lifeless , hackneyed drivel like this than to expend any real effort . unfortunately , it's movies like this that give the genre a bad name . in this case , a very bad name . ultimately , sitting through the mangler is about as appealing as getting squeezed through the machine . \n0\n"
    }
   ],
   "source": [
    "reviews = []\n",
    "ratings = []\n",
    "\n",
    "dataset_dir = 'cornell_nlp_dataset/scaledata'\n",
    "folders = os.listdir(dataset_dir)\n",
    "for folder in folders:\n",
    "    fname = os.path.join(dataset_dir, str(folder)+'/'+'subj.'+str(folder))\n",
    "    f = open(fname)\n",
    "    data = f.read()\n",
    "    lines = data.split('\\n')\n",
    "    reviews.extend(lines)\n",
    "    reviews = reviews[:-1]\n",
    "    f.close()\n",
    "    fname2 = os.path.join(dataset_dir, str(folder)+'/'+'label.3class.'+str(folder))\n",
    "    f2 = open(fname2)\n",
    "    data2 = f2.read()\n",
    "    rating = data2.split('\\n')\n",
    "    ratings.extend(rating)\n",
    "    ratings = ratings[:-1]\n",
    "    ratings = [int(rat) for rat in ratings]\n",
    "    f2.close()\n",
    "print(reviews[1])\n",
    "print(ratings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No. of reviews in this dataset: 5006\n"
    }
   ],
   "source": [
    "print('No. of reviews in this dataset: %d' %len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No. of unique tokens: 46259\nShape of dataset:  (5006, 400)\nShape of labels:  (5006,)\n"
    }
   ],
   "source": [
    "maxlen = 400\n",
    "training_samples = 4006\n",
    "val_samples = 1000\n",
    "test_samples = 1000\n",
    "max_words = 15000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print('No. of unique tokens: %d' %(len(word_index)))\n",
    "\n",
    "dataset = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(ratings)\n",
    "\n",
    "print('Shape of dataset: ', dataset.shape)\n",
    "print('Shape of labels: ', labels.shape)\n",
    "\n",
    "indices = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "dataset = dataset[indices]\n",
    "labels = labels[indices]\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "x_train = dataset[:training_samples]\n",
    "y_train = labels[:training_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 400, 50)           750000    \n_________________________________________________________________\nspatial_dropout1d_3 (Spatial (None, 400, 50)           0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 394, 64)           22464     \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 78, 64)            0         \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 78, 64)            24832     \n_________________________________________________________________\nbidirectional_4 (Bidirection (None, 128)               66048     \n_________________________________________________________________\ndense_3 (Dense)              (None, 3)                 387       \n=================================================================\nTotal params: 863,731\nTrainable params: 863,731\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 50, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Conv1D(64, 7, activation='relu'))\n",
    "model.add(MaxPool1D(5))\n",
    "model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 3004 samples, validate on 1002 samples\nEpoch 1/40\n3004/3004 [==============================] - 15s 5ms/sample - loss: 1.0787 - acc: 0.4005 - val_loss: 1.0693 - val_acc: 0.4112\nEpoch 2/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 1.0491 - acc: 0.4271 - val_loss: 0.9928 - val_acc: 0.4990\nEpoch 3/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.8372 - acc: 0.5869 - val_loss: 0.8056 - val_acc: 0.5888\nEpoch 4/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.5828 - acc: 0.7397 - val_loss: 0.8435 - val_acc: 0.6188\nEpoch 5/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.4048 - acc: 0.8379 - val_loss: 0.9774 - val_acc: 0.6108\nEpoch 6/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.2584 - acc: 0.9101 - val_loss: 1.2256 - val_acc: 0.6068\nEpoch 7/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.1704 - acc: 0.9358 - val_loss: 1.4346 - val_acc: 0.6108\nEpoch 8/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.1709 - acc: 0.9457 - val_loss: 1.3871 - val_acc: 0.6038\nEpoch 9/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.1165 - acc: 0.9654 - val_loss: 1.3829 - val_acc: 0.6058\nEpoch 10/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.1014 - acc: 0.9667 - val_loss: 1.5051 - val_acc: 0.6028\nEpoch 11/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.0799 - acc: 0.9780 - val_loss: 1.6516 - val_acc: 0.6098\nEpoch 12/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.0516 - acc: 0.9883 - val_loss: 1.7186 - val_acc: 0.6098\nEpoch 13/40\n3004/3004 [==============================] - 10s 3ms/sample - loss: 0.0692 - acc: 0.9794 - val_loss: 1.8343 - val_acc: 0.5958\n"
    }
   ],
   "source": [
    "callbacks_list = [EarlyStopping(monitor='acc', patience=1), ModelCheckpoint(monitor='val_loss', filepath='trained_model.h5', save_best_only=True)]\n",
    "history = model.fit(x_train, y_train, epochs=40, batch_size=64, validation_split=0.25, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}